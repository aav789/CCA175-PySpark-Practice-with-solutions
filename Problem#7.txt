Instructions
	List the names of the Top 5 products by revenue ordered on '2013-07-26'. 
	Revenue is considered only for COMPLETE and CLOSED orders.
Data Description
	retail_db data is available in HDFS at /public/retail_db
	Note:Download this data from https://github.com/ramapilli16/data or https://github.com/dgadiraju/data

retail_db data information:
	Source directories:
	/public/retail_db/orders
	/public/retail_db/order_items
	/public/retail_db/products
	
	Source delimiter: comma(",")
	Source Columns - orders - order_id, order_date, order_customer_id,
	order_status
	Source Columns - order_itemss - order_item_id, order_item_order_id,
	order_item_product_id, order_item_quantity, order_item_subtotal,
	order_item_product_price
	Source Columns - products - product_id, product_category_id, product_name,
	product_description, product_price, product_image
Output Requirements
	Target Columns: order_date, order_revenue, product_name,
	product_category_id
	Data has to be sorted in descending order by order_revenue
	File Format: text
	Delimiter: colon (:)
	Place the output file in the HDFS directory
	/user/`whoami`/problem7/solution/
	Replace `whoami` with your OS user name
End of Problem

Solution: 
#using Spark DF APIs
	
	#use show(7,False), printSchema() to check output in every step
	#import all the necessary functions 
	from pyspark.sql.functions import substring, sum,desc, concat, dense_rank
	from pyspark.sql.window import *
	
	#create DFs #Note: I have not given schema for OrderItems and products as I don't need all columns.

	orders=spark.read.format('csv').option("delimter",",").schema("order_id int, order_date string, order_customer_id int,order_status string").load("/public/data/retail_db/orders/")
	orderItems=spark.read.format('csv').option("delimiter", ",").option("inferSchema", True).load("/public/data/retail_db/order_items/")
	products=spark.read.format('csv').option("delimiter", ",").option("inferSchema", True).load("/public/data/retail_db/products/")

	#filtering and column naming
	#try to keep your dataset as simple as posisble by appliying the filters as needed and select only required columns
	
	of=orders.selectExpr("order_id", "substring(order_date,1,10) as order_date", "order_status").where("order_date='2013-07-26' AND order_status IN ('COMPLETE', 'CLOSED')")
	oi=orderItems.selectExpr("_c1 as oi_order_id", "_c2 as oi_pid", "_c4 as oi_subtotal")
	pf=products.selectExpr("_c0 as pid", "_c1 as product_category_id", "_c2 as product_name")
	
	oi.printSchema()
	#join and aggregate 
	#to find revenue by per product per day 
	oj=of.join(oi, oi.oi_order_id==of.order_id, "inner").groupBy(oi.oi_pid, of.order_date).agg(sum(oi.oi_subtotal).alias('order_revenue'))
	
	#join and window
	#join with product and apply window functions to get top 5 
	opj=oj.join(pf, oj.oi_pid==pf.pid, "inner").withColumn('drank', dense_rank().over(Window.orderBy(desc('order_revenue')))).where("drank <=5")

	opj.show(7,False)

	#create result and save
	result=opj.orderBy('order_revenue', ascending =False).selectExpr("concat(order_date,':', round(order_revenue,2), ':', product_name, ':', product_category_id)")
	result.coalesce(1).write.mode("overwrite").format("text").save("/public/data/output/sol7/")

	#validate
	import os
	os.system("hdfs dfs -ls /public/data/output/sol7/")
	os.system("hdfs dfs -cat /public/data/output/sol7/part-00000-66562c9a-e70f-45d2-b61b-b312e2fcbab5-c000.txt | more")
	oj.selectExpr("max(order_revenue)") #gets max value
	
	#if you are running hadoop commands in another terminal you don't need to import os. -head is not working with os.
	hdfs dfs -head /public/data/output/sol7/part-00000-66562c9a-e70f-45d2-b61b-b312e2fcbab5-c000.txt
	
#Notes:
I didn't round order_revenue in early aggregation as it defaulted to double and round expects float
I have used concat as it required to store the file in text format and multiple columns are not supported.
If it is csv, you don't need to concat
I have used dense_rank with an assumption that many products may have same revenue


#using SparkSQL
#if you are good at sql after creating DFs create temporary tables write sql 
	oi.createOrReplaceTempView("oi")
	of.createOrReplaceTempView("of")
	pf.createOrReplaceTempView("pf")
	
	oj=spark.sql("select round(sum(oi.oi_subtotal),2) as order_revenue, oi_pid, order_date from of inner join oi on of.order_id=oi.oi_order_id group by oi.oi_pid, of.order_date")
	oj.createOrReplaceTempView("oj")
	
	result=spark.sql("select concat(T.order_date, ':', T.order_revenue, ':', pf.product_name, ':', pf.product_category_id) as result from \
(select * from (select *, dense_rank() over(order by order_revenue desc) as drank from oj) T1 WHERE T1.drank <=5)T \
inner join pf on pf.pid=T.oi_pid order by T.order_revenue desc")




	
